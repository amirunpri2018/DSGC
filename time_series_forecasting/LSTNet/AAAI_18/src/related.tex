\section{Related Background}
\label{sec:related}

%Peter 
%Time series modeling, also known as linear dynamic system or system identification in the field of control theory \cite{ljung1999system}, aims at the creation of a model based on measured signals. This model can be used, amongst other things, to predict future behavior of the system, to explain interesting structures in the data or to denoise the original time series. 
%In this paper, we are concerned with modeling multivariate time series where a set of observations are associated with multiple output variables. This is in contrast to univariate time series consisting of a set of observations on a single output.
%Time series forecasting models have been applied to a broad raunge of domains, including economic planning and financial markets \cite{azoff1994neural, kim2003financial, granger2014forecasting}, energy consumption \cite{abdel1997forecasting, sfetsos2000comparison}, and various branches of science \cite{jain2007hybrid,reikard2009predicting}. From the perspective of machine learning, time series forecasting can be formulated as a regression task. Existing regression methods for time series forecasting can be categorized into three different ways: linear models, non-linear models, and deep learning models.

One of the most prominent univariate time series models is the autoregressive integrated moving average (ARIMA) model. The popularity of the ARIMA model is due to its statistical properties as well as the well-known Box-Jenkins methodology \cite{box2015time} in the model selection procedure. ARIMA models are not only adaptive to various exponential smoothing techniques \cite{mckenzie1984general} but also flexible enough to subsume other types of time series models including autoregression (AR), moving average (MA) and Autoregressive Moving Average (ARMA). However, ARIMA models, including their variants for modeling long-term temporal dependencies \cite{box2015time}, are rarely used in high dimensional multivariate time series forecasting due to their high computational cost. %Specifically, even the simplest AR model of order $p$ requires $O(Tp^2n^4 + p^3n^6)$ to estimate $O(pn^2)$ parameters. This time complexity is practically infeasible for large number of variables $n$, not to mention other advanced extensions such as ARIMA and seasonal ARIMA.

On the other hand, vector autoregression (VAR) is arguably the most widely used models in multivariate time series
\cite{hamilton1994time,box2015time,lutkepohl2005new} due to its simplicity. VAR models naturally extend AR models to the multivariate setting, which ignores the dependencies between output variables. Significant progress has been made in recent years in a variety of VAR models, including the elliptical VAR model \cite{qiu2015robust} for heavy-tail time series and structured VAR model \cite{melnyk2016estimating} for better interpretations of the dependencies between high dimensional variables, and more. Nevertheless, the model capacity of VAR grows linearly over the temporal window size and quadratically over the number of variables. This implies, when dealing with long-term temporal patterns, the inherited large model is prone to overfitting. To alleviate this issue, \cite{Yu_NIPS_16} proposed to reduce the original high dimensional signals into lower dimensional hidden representations, then applied VAR for forecasting with a variety choice of regularization.

Time series forecasting problems can also be treated as standard regression problems with time-varying parameters. It is therefore not surprising that various regression models with different loss functions and regularization terms are applied to time series forecasting tasks. For example, linear support vector regression (SVR) \cite{kim2003financial,cao2003support} learns a max margin hyperplane based on the regression loss with a hyper-parameter $\epsilon$ controlling the threshold of prediction errors. Ridge regression is yet another example which can be recovered from SVR models by setting $\epsilon$ to zeros. Lastly, \cite{li2014forecasting} applied LASSO models to encourage sparsity in the model parameters so that interesting patterns among different input signals could be manifest. These linear methods are practically more efficient for multivariate time series forecasting due to high-quality off-the-shelf solvers in the machine learning community. Nonetheless, like VARs, those linear models may fail to capture complex non-linear relationships of multivariate signals, resulting in an inferior performance at the cost of its efficiency.

Gaussian Processes (GP) is a non-parametric method for modeling distributions over a continuous domain of functions. This contrasts with models defined by a parameterized class of functions such as VARs and SVRs. GP can be applied to multivariate time series forecasting task as suggested in \cite{roberts2013gaussian}, and can be used as a prior over the function space in Bayesian inference. For example, \cite{frigola2013bayesian} presented a fully Bayesian approach with the GP prior for nonlinear state-space models, which is capable of capturing complex dynamical phenomena. However, the power of Gaussian Process comes with the price of high computation complexity. A straightforward implementation of Gaussian Process for multivariate time-series forecasting has cubic complexity over the number of observations, due to the matrix inversion of the kernel matrix.

%Guokun
%	Recently, Recurrent Neural Networks (RNN)\cite{elman1990finding} and Convolution Neural Networks (CNN)\cite{lecun1995convolutional} have yielded start-of-the-art results across a lot of applications \cite{bahdanau2014neural,hinton2012deep,krizhevsky2012imagenet}. For time series data, researchers have explored them in different related tasks. For instance, the RNN structure has been applied in diagnosis based on extracted patterns out of health-care time series data \cite{lipton2015learning,che2016recurrent} and action/activity detection from mobile data   \cite{hammerla2016deep}. Dasgupta etc.\cite{dasgupta2016nonlinear} has leveraged vanilla RNN and Dynamic Boltzmann Machines for time series prediction and showed results comparable to the LSTM network. And researchers have mainly exploited the CNN structure in action/activity recognition\cite{lea2016temporal,yang2015deep,hammerla2016deep} by utilizing its salient ability to mine the time invariant patterns in time series data.  However, to the best of our knowledge, no work has been conducted to examine how to utilize the RNN(LSTM and GRU) and CNN structures efficiently in the task of multivariate time series forecasting or compare it with the classical time series models.
    
%    Moreover, artificial neural network models had gained great success in univariate time series forecasting problem a decade ago\cite{zhang1998forecasting,zhang2003time,jain2007hybrid}. But researchers only focused on combining ARIMA\cite{box1970distribution} model with Multilayer Perceptron (MLP). However, they only adopted the most naive version among neural network models. Given the tremendous development of the advanced CNN and RNN architectures in these years, it is time to redesigned a more efficient neural network architecture for the time series forecasting task.
    
%Last, another line of research related to time series forecasting is time series classification. \peter{TODO: introduce some time series classification related works.}
