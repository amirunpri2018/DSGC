\section{Framework}
\label{sec:model}

In this section, we first formulate the time series forecasting problem, and then discuss the details of the proposed LSTNet architecture (Figure \ref{fig:overview}) in the following part. Finally, we introduce the objective function and the optimization strategy.
\subsection{Problem Formulation}
\label{sec:format}

In this paper, we are interested in the task of multivariate time series forecasting. More formally, given a series of fully observed time series signals $\bY = \{\by_1, \by_2, \ldots, \by_T \}$ where $\by_t \in \R^n$, and $n$ is the variable dimension, we aim at predicting a series of future signals in a rolling forecasting fashion. That being said, to predict $\by_{T+h}$ where $h$ is the desirable horizon ahead of the current time stamp, we assume $\{ \by_1, \by_2, \ldots, \by_T \}$ are available. Likewise, to predict the  value of the next time stamp $\by_{T+h+1}$, we assume $\{ \by_1, \by_2, \ldots, \by_T, \by_{T+1}\}$ are available. We hence formulate the input matrix at time stamp $T$ as $X_T = \{ \by_1, \by_2, \ldots, \by_T\} \in \R^{n \times T}$. 

In the most of cases, the horizon of the forecasting task is chosen according to the demands of the environmental settings, e.g. for the traffic usage, the horizon of interest ranges from hours to a day; for the stock market data, even seconds/minutes-ahead forecast can be meaningful for generating returns.

Figure \ref{fig:overview} presents an overview of the proposed LSTnet architecture. The LSTNet is a deep learning framework specifically designed for multivariate time series forecasting tasks with a mixture of long- and short-term patterns. In following sections, we introduce the building blocks for the LSTNet in detail. 

\subsection{Convolutional Component}
The first layer of LSTNet is a convolutional network without pooling, which aims to extract short-term patterns in the time dimension as well as local dependencies between variables. The convolutional layer consists of multiple filters
%and its $k$-th filter is parameterized by $W_k \in \R^{n \times \omega}$ with a bias term $b_k$,
of width $\omega$ and height $n$ (the height is set to be the same as the number of variables). The $k$-th filter sweeps through the input matrix $X$ and produces
\begin{equation}
h_k = RELU(W_k * X + b_k)
\end{equation}
where $*$ denotes the convolution operation and the output $h_k$ would be a vector, and the $RELU$ function is $RELU(x) = \max(0,x)$. We make each vector $h_k$ of length $T$ by zero-padding on the left of input matrix $X$. The output matrix of the convolutional layer is of size $d_c \times T$
where $d_c$ denotes the number of filters.

\subsection{Recurrent Component}
The output of the convolutional layer is simultaneously fed into the Recurrent component and Recurrent-skip component (to be described in subsection \ref{subsec:recurrent-skip}). The Recurrent component is a recurrent layer with the Gated Recurrent Unit (GRU) \cite{chung2014empirical} and uses the $RELU$ function as the hidden update activation function. The hidden state of recurrent units at time $t$ is computed as,

\begin{equation}
\begin{aligned}
r_t &= \sigma(x_tW_{xr} + h_{t-1}W_{hr} + b_r) \\
u_t &= \sigma(x_tW_{xu} + h_{t-1}W_{hu} + b_u) \\
c_t &= RELU(x_tW_{xc} + r_{t}\odot(h_{t-1}W_{hc}) + b_c) \\
h_t &= (1-u_t) \odot h_{t-1} + u_t \odot c_t \\
\end{aligned}
\label{eq:gru}
\end{equation}

where $\odot$ is the element-wise product, $\sigma$ is the sigmoid function and $x_t$ is the input of this layer at time $t$. The output of this layer is the hidden state at each time stamp. While researchers are accustomed to using $\tanh$ function as hidden update activation function, we empirically found $RELU$ leads to more reliable performance, through which the gradient is easier to back propagate. 

\subsection{Recurrent-skip Component}
\label{subsec:recurrent-skip}

The Recurrent layers with GRU \cite{chung2014empirical} and LSTM \cite{hochreiter1997long} unit are carefully designed to memorize the historical information and hence to be aware of relatively long-term dependencies. Due to gradient vanishing, however, GRU and LSTM usually fail to capture very long-term correlation in practice. We propose to alleviate this issue via a novel recurrent-skip component which leverages the periodic pattern in real-world sets. For instance, both the electricity consumption and traffic usage exhibit clear pattern on a daily basis. If we want to predict the electricity consumption at $t$ o'clock for today, a classical trick in the seasonal forecasting model is to leverage the records at $t$ o'clock in historical days, besides the most recent records.
% The intuition here is that signals at the same phase before several periods may have strong correlation to the present signal.
This type of dependencies can hardly be captured by off-the-shelf recurrent units due to the extremely long length of one period (24 hours) and the subsequent optimization issues. Inspired by the effectiveness of this trick, we develop a recurrent structure with temporal skip-connections to extend the temporal span of the information flow and hence to ease the optimization process. Specifically, skip-links are added between the current hidden cell and the hidden cells in the same phase in adjacent periods. The updating process can be formulated as,

\begin{equation}
\begin{aligned}
r_t &= \sigma(x_tW_{xr} + h_{t-p}W_{hr} + b_r) \\
u_t &= \sigma(x_tW_{xu} + h_{t-p}W_{hu} + b_u) \\
c_t &= RELU(x_tW_{xc} + r_{t}\odot(h_{t-p}W_{hc}) + b_c) \\
h_t &= (1-u_t) \odot h_{t-p} + u_t \odot c_t \\
\end{aligned}
\label{eq:rnn-skip}
\end{equation}

where the input of this layer is the output of the convolutional layer, and $p$ is the number of hidden cells skipped through. The value of $p$ can be easily determined for datasets with clear periodic patterns (e.g. $p=24$ for the hourly electricity consumption and traffic usage datasets), and has to be tuned otherwise. In our experiments, we empirically found that a well-tuned $p$ can considerably boost the model performance even for the latter case. Furthermore, the LSTNet could be easily extended to contain variants of the skip length $p$.

We use a dense layer to combine the outputs of the Recurrent and Recurrent-skip components. The inputs to the dense layer include the hidden state of Recurrent component at time stamp $t$, denoted by $h^R_t$, and $p$ hidden states of Recurrent-skip component from time stamp $t-p+1$ to $t$ denoted by $h^S_{t-p+1},h^S_{t-p+2} \ldots, h^S_{t}$. The output of the dense layer is computed as,

\begin{equation}
h^D_t = W^R h^R_t + \sum_{i=0}^{p-1} W^S_{i}h^S_{t-i} + b
\label{eq:dense}
\end{equation}

where $h^D_t$ is the prediction result of the neural network (upper) part in the Fig.\ref{fig:overview} at time stamp $t$.

\subsection{Autoregressive Component}
\label{sec:AR}
Due to the non-linear nature of the Convolutional and Recurrent components, one major drawback of the neural network model is that the scale of outputs is not sensitive to the scale of inputs. Unfortunately, in specific real datasets, the scale of input signals constantly changes in a non-periodic manner, which significantly lowers the forecasting accuracy of the neural network model. A concrete example of this failure is given in Section \ref{sec:ablation}. To address this deficiency, similar in spirit to the highway network \cite{srivastava2015highway}, we decompose the final prediction of LSTNet into a linear part, which primarily focuses on the local scaling issue, plus a non-linear part containing recurring patterns. In the LSTNet architecture, we adopt the classical Autoregressive (AR) model as the linear component. Denote the forecasting result of the AR component as $h^L_{t} \in \R^n$, and the coefficients of the AR model as $W^{ar} \in \R^{q^{ar}}$ and $b^{ar} \in \R$, where $q^{ar}$ is the size of input window over the input matrix. Note that in our model, all dimensions share the same set of linear parameters. The AR model is formulated as follows,
\begin{equation}
h^L_{t,i} = \sum_{k=0}^{q^{ar}-1}W^{ar}_k\by_{t-k,i} + b^{ar}
\end{equation}

The final prediction of LSTNet is then obtained by by integrating the outputs of the neural network part and the AR component:
\begin{equation}
\hat{\bY}_t = h^D_t + h^L_t
\end{equation}
where $\hat{\bY}_t$ denotes the model's final prediction at time stamp $t$.

\subsection{Objective function}
The squared error is the default loss function for many forecasting tasks,
the corresponding optimization objective is formulated as,
\begin{equation}
\minimize_{\Theta}~~~\sum_{t \in \Omega_{Train}} ||\bY_t - \hat{\bY}_{t-h}||_F^2
\label{eq:L2loss}
\end{equation}
where $\Theta$ denotes the parameter set of our model, $\Omega_{Train}$ is the set of time stamps used for training, $||\cdot||_F$ is the Frobenius norm, and $h$ is the horizon as mentioned in Section \ref{sec:format}.
The traditional linear regression model with the square loss function is named as Linear Ridge, which is equivalent to the vector autoregressive model with ridge regularization. However, experiments show that the Linear Support Vector Regression (Linear SVR) \cite{vapnik1997support} dominates the Linear Ridge model in certain datasets. The only difference between Linear SVR and Linear Ridge is the objective function. The objective function for Linear SVR is,

\begin{equation}
\begin{aligned}
\minimize_{\Theta}~~~&\frac{1}{2}||\Theta||_F^2 + C\sum_{t \in \Omega_{Train}} \sum_{i=0}^{n-1} \xi_{t,i} \\
\st~~~& |\hat{\bY}_{t-h,i} - \bY_{t,i}| \le \xi_{t,i} + \epsilon, t \in \Omega_{Train} \\
& \xi_{t,i} \ge 0
\end{aligned}
\label{eq:svr}
\end{equation}
where $C$ and $\epsilon$ are hyper-parameters. Motivated by the remarkable performance of the Linear SVR model, we incorporate its objective function in the LSTNet model as an alternative of the squared loss. For simplicity, we assume $\epsilon=0$\footnote{One could keep $\epsilon$ to make the objective function more faithful to the Linear SVR model without modifying the optimization strategy. We leave this for future study.}, and the objective function above reduces to absolute loss (L1-loss) function as follows:
\begin{equation}
\minimize_{\Theta}~~~\sum_{t \in \Omega_{Train}} \sum_{i=0}^{n-1}|\bY_{t,i} - \hat{\bY}_{t-h,i}|
\label{eq:L1loss}
\end{equation}

In the experiment section, we carefully examine the effectiveness of both objective functions defined in Equation \ref{eq:L2loss} and Equation \ref{eq:L1loss}.

\subsection{Optimization Strategy}
\label{sec:train}

\iffalse
For a time series sequence, the most straightforward training strategy is to treat the problem as a supervision task with an input time series signals and an output time series signals. Its advantage lies in efficient computation, which allows for going though the neural network part one time at each iteration. But the defect is that we only have one training sample. That being said, we cannot apply stochastic gradient decent (SGD) method. Given the highly non-convex nature of neural network objective function, this strategy can easily lead to bad performance, which is stick at the bad local minimal. 
\fi

%In the following, we introduce the way to optimize the MTNN structure. 
Our optimization strategy is the same as that in the traditional time series forecasting model. Supposing the input time series is $\bY_t = \{\by_1, \by_2, \ldots, \by_t \}$, we define a tunable window size $q$, and reformulate the input at time stamp $t$ as $\bX_t = \{\by_{t-q+1}, \by_{t-q+2}, \ldots, \by_t \}$. The problem then becomes a regression task with a set of feature-value pairs $\{\bX_t, \bY_{t+h}\}$, and can be solved by Stochastic Gradient Decent (SGD) or its variants such as Adam \cite{kingma2014adam}.


